---
title: "Diagnostic Survey Weight Tests"
format: html
editor: visual
---

### Set-up

```{r, message = FALSE, warning = FALSE}
if (!require(tidyverse)) install.packages("tidyverse"); library(tidyverse)
if (!require(rpms)) install.packages("rpms"); library(rpms)
if (!require(sampling)) install.packages("sampling"); library(sampling)
if (!require(survey)) install.packages("survey"); library(survey)
```

```{r}
generate_data_study1 = function(N, sigma, alpha, delta) {
  X <- runif(N, 0, 1)
  u <- runif(N, 0, 1)
  epsilon <- rnorm(N, 0, sd = sigma)
  
  Y <- 1 + X + epsilon
  w <- alpha * Y + 0.3 * X + delta * u
  data = data.frame(y = Y, x = X, w)
  return(data)
}

generate_data_study2 = function(N, sigma, alpha) {
  X <- runif(N, 0, 1)
  u <- runif(N, 0, 1)
  epsilon <- rnorm(N, 0, sd = sigma)
  
  Y <- 1 + X + epsilon
  w <- alpha * (Y - 1.5 * alpha)^2 + 0.3 * X - 0.3 * X^2 + u
  data = data.frame(y = Y, x = X, w)
  return(data)
}

# Make Study 3 data generation
generate_data_study3 = function(N, alpha, psi) {
  X <- rnorm(N, 0, sd = sqrt(0.5))
  epsilon <- rnorm(N, 0, sd = sqrt(0.5))
  z <- rnorm(N, 0, sd = sqrt(0.5))
  beta = 2 - alpha
  
  eta = function(x) {
    ifelse(x < 0.2, 0.025,
               ifelse(0.2 <= x & x <= 1.2, 0.475 * (x - 0.2) + 0.025,
               0.5))
  }
  
  Y <- 0.5 + X + epsilon
  w <- alpha * eta(X) + beta * eta(psi * epsilon + (1 - psi) * z)
  data = data.frame(y = Y, x = X, w)
  return(data)
}

# sigma = sqrt(0.5)
# study3_integral = function(x) {
#   (x / sigma) / sqrt(2 * pi) * exp(-(x / sigma)^2 / 2)
# }
# integrate(study3_integral, lower = 0.2, upper = 1.2)
# 
# study3_integral(x = X)

generate_sample_brewer = function(data, w, n, rescale = FALSE) {
  pik = inclusionprobabilities(w, n)
  choosen = UPbrewer(pik)
  samp = data[1:nrow(data) * choosen,] %>%
    mutate(w = 1 / pik[1:nrow(data) * choosen])
  if (rescale == TRUE) mutate(samp, w = w / sum(w))
  return(samp)
}

# Make Study 3 data sampling via poisson sampling
generate_sample_poisson = function(data, w, n, rescale = FALSE) {
  choosen = as.numeric(runif(length(w), 0, (1 / n) * sum(w)) < w)
  samp = cbind(data, choosen) %>%
    filter(choosen == 1) %>%
    select(-choosen) %>%
    mutate(w = 1 / w) # Redefine from pi to weights w
  if (rescale == TRUE) mutate(samp, w = w / sum(w))
  return(samp)
}

generate_sample_poisson_suspicious = function(data, w, n, rescale = FALSE) {
  choosen = as.numeric(runif(length(w), 0, (1 / n) * sum(w)) < w)
  samp = cbind(data, choosen) %>%
    filter(choosen == 1) %>%
    select(-choosen) # Redefine from pi to weights w
  if (rescale == TRUE) mutate(samp, w = w / sum(w))
  return(samp)
}

generate_sample_brewer_suspicious = function(data, w, n, rescale = FALSE) {
  pik = inclusionprobabilities(w, n)
  choosen = UPbrewer(pik)
  samp = data[1:nrow(data) * choosen,]
  if (rescale == TRUE) mutate(samp, w = w / sum(w))
  return(samp)
}
```

```{r}
N <- 3000
n <- c(100, 200)
sigma <- c(0.1, 0.2)
alpha <- c(0.0, 0.2, 0.4, 0.6)
delta <- c(1.0, 1.5)

cases <- expand_grid(N, n, sigma, delta, alpha)

m = 1
pop = generate_data_study1(N = cases$N[m],
                           sigma = cases$sigma[m],
                           alpha = cases$alpha[m],
                           delta = cases$delta[m])

# Keeping all units to test out the test code
samp = generate_sample_brewer(data = pop,
                              w = pop$w, 
                              n = cases$n[m], #nrow(pop) / 2,
                              rescale = FALSE) #cases$n[m])

#samp = pop %>%
#  mutate(w = 1 / inclusionprobabilities(w, nrow(pop) / 2)) %>%
#  sample_n(size = 1500)
```


# Difference-in-Coefficients Tests

### Hausman-Pfeffermann DC Test (adapted)

```{r}
# TO-DO: incorporate categorical variables
HP_DC_test = function(data, y, x, wts) {
  # Unweighted Regression --------------
  X = cbind(1, x)
  betas_u = solve(t(X) %*% X) %*% t(X) %*% y
  
  # Weighted Regression ----------------
  W =  diag(x = wts, nrow = length(wts), ncol = length(wts))
  betas_w = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y
  
  # Calculate Test Statistic -----------

  # Calculate sigma^2 estimate from OLS under null
  y_hat = X %*% betas_u
  residuals = y - y_hat
  SSE = sum(residuals^2)
  sigma_sq_hat = SSE / (length(y) - length(betas_u) - 1)
  
  # A (note that in paper, H = diag(w) which is our W)
  A = (solve(t(X) %*% W %*% X) %*% t(X) %*% W) - (solve(t(X) %*% X) %*% t(X))
  
  # Calculate variance
  V_hat = sigma_sq_hat * A %*% t(A)
  
  # Chi Squared Test Statistic
  Chi_statistic = t(betas_w - betas_u) %*% solve(V_hat) %*% (betas_w - betas_u)
  p_value = pchisq(q = Chi_statistic, df = length(betas_u), lower.tail = FALSE)
  return(p_value)
}

HP_DC_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```


# Weight Association Tests

### DuMouchel-Duncan WA Test

```{r}
DD_WA_test = function(data, y, x, wts) {
  # Create matrices
  X = cbind(1, x)
  W = diag(x = wts, ncol = length(wts), nrow = length(wts))
  X_tilde = W %*% X 
  X_comb = cbind(X, X_tilde)
  
  # Full model
  betas_comb = solve(t(X_comb) %*% X_comb) %*% t(X_comb) %*% y
  y_hat_full = X_comb %*% betas_comb
  RSS_full = sum((y - y_hat_full)^2)
  
  # Reduced model
  X_reduced = X_comb[, c(1,2)]
  betas_reduced = solve(t(X_reduced) %*% X_reduced) %*% t(X_reduced) %*% y
  y_hat_reduced = X_reduced %*% betas_reduced
  RSS_reduced = sum((y - y_hat_reduced)^2)
  
  # F-test
  F_statistic = (RSS_reduced - RSS_full) / (length(betas_comb) - length(betas_reduced)) /
    (RSS_full / (length(y) - ncol(X_comb)))
  p_value = 1 - pf(F_statistic, df1 = length(betas_comb) - length(betas_reduced), df2 = length(y) - ncol(X_comb))
  return(p_value)
}

DD_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```

### Pfeffermann-Sverchkov WA Test 1 - Correlation Testing

They propose a simple modification by regressing $W$ on the first two moments and an interaction with $X$: $$E(W \mid \hat{\epsilon}_u) = f(X; \eta) + \sum_{k = 1}^2 \beta^{(k)} \hat{\epsilon}_u^k + \text{diag}(\hat{\epsilon}_u)X \gamma,$$ where $f(X;\eta)$ is a function of $X$ with scalar parameter $\eta$, scalar coefficients $\beta^{(1)}$ and $\beta^{(2)}$, and $\gamma$ is a $p \times 1$ coefficient vector for the interaction between $X$ and $\hat{\epsilon}$. Finally, test the null hypothesis $H_0: \beta^{(1)} = \beta^{(2)} = \gamma = 0$ by an $F$-test.

```{r}
PS1_WA_test = function(data, y, x, wts) {
  W = wts
  
  X = cbind(1, x)
  betas_u = solve(t(X) %*% X) %*% t(X) %*% y
  
  y_hat = X %*% betas_u
  residuals = y - y_hat
  residuals_diag = diag(x = as.vector(residuals), ncol = length(residuals), nrow = length(residuals))
  
  # Create Full Model
  E = residuals
  E_sq = residuals^2
  X_tilde = residuals_diag %*% x
  X_design = cbind(1, x, E, E_sq, X_tilde)
  betas_full = solve(t(X_design) %*% X_design) %*% t(X_design) %*% W
  W_hat_full = X_design %*% betas_full
  RSS_full = sum((W - W_hat_full)^2)
  
  # Create Reduced Model
  X_reduced = cbind(1, x)
  betas_reduced = solve(t(X_reduced) %*% X_reduced) %*% t(X_reduced) %*% W
  W_hat_reduced = X_reduced %*% betas_reduced
  RSS_reduced = sum((W - W_hat_reduced)^2)
  
  # F-test
  F_statistic = ((RSS_reduced - RSS_full) /
                   (length(betas_full) - length(betas_reduced))) /
      (RSS_full / (length(W) - length(betas_full)))
    p_value = 1 - pf(F_statistic,
                     df1 = length(betas_full) - length(betas_reduced),
                     df2 = length(W) - length(betas_full))
  return(p_value)
}

PS1_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```

```{r}
PS1q_WA_test = function(data, y, x, wts) {
  W = wts
  
  X = cbind(1, x)
  betas_u = solve(t(X) %*% X) %*% t(X) %*% y
  
  y_hat = X %*% betas_u
  residuals = y - y_hat
  residuals_diag = diag(x = as.vector(residuals), ncol = length(residuals), nrow = length(residuals))
  
  # Create Full Model
  X_sq = x^2
  E = residuals
  E_sq = residuals^2
  X_tilde = residuals_diag %*% x
  X_design = cbind(1, x, X_sq, E, E_sq, X_tilde)
  betas_full = solve(t(X_design) %*% X_design) %*% t(X_design) %*% W
  W_hat_full = X_design %*% betas_full
  RSS_full = sum((W - W_hat_full)^2)
  
  # Create Reduced Model
  X_reduced = cbind(1, x)
  betas_reduced = solve(t(X_reduced) %*% X_reduced) %*% t(X_reduced) %*% W
  W_hat_reduced = X_reduced %*% betas_reduced
  RSS_reduced = sum((W - W_hat_reduced)^2)
  
  # F-test
  F_statistic = ((RSS_reduced - RSS_full) /
                   (length(betas_full) - length(betas_reduced))) /
      (RSS_full / (length(W) - length(betas_full)))
    p_value = 1 - pf(F_statistic,
                     df1 = length(betas_full) - length(betas_reduced),
                     df2 = length(W) - length(betas_full))
  return(p_value)
}

PS1q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```



### Pfeffermann-Sverchkov WA Test 2 - Weight Informative

Wang \textit{et al.} (2023) critiques the regression model $E(W \mid X, Y)$ since it would only capture a linear relationship between $W$ and $(X,Y)$. Thus, they suggest capturing possible non-linear relationships by considering $$E(W \mid X, Y) = f(X; \eta) + \sum_{k = 1}^2 Y^k \gamma_k,$$ where $f(X; \eta)$ is a function of $X$ with parameter $\eta$, coefficient $\gamma_k$ of $Y^k$. Finally, test the null hypothesis $H_0: \gamma_1 = \gamma_2 = 0$ with an $F$-test to determine whether $W$ and $Y$ are associated conditional on $X$.

```{r}
PS2_WA_test = function(data, y, x, wts) {
  W = wts
  
  # Full model: Regressing W on both X and Y with quadratic terms 
  XY_full = cbind(1, x, y, y^2)
  betas_full = solve(t(XY_full) %*% XY_full) %*% t(XY_full) %*% W
  w_hat_full = XY_full %*% betas_full
  RSS_full = sum((W - w_hat_full)^2)
  
  # Reduced model: Regressing W only on X and X^2
  XY_reduced = cbind(1, x)
  betas_reduced = solve(t(XY_reduced) %*% XY_reduced) %*% t(XY_reduced) %*% W
  w_hat_reduced = XY_reduced %*% betas_reduced
  RSS_reduced = sum((W - w_hat_reduced)^2)
  
  # F-test
  F_statistic = ((RSS_reduced - RSS_full) /
                   (length(betas_full) - length(betas_reduced))) /
    (RSS_full / (length(W) - length(betas_full)))
  p_value = 1 - pf(F_statistic,
                   df1 = length(betas_full) - length(betas_reduced),
                   df2 = length(W) - length(betas_full))
  return(p_value)
}

PS2_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```

```{r}
PS2q_WA_test = function(data, y, x, wts) {
  W = wts
  
  # Full model: Regressing W on both X and Y with quadratic terms 
  XY_full = cbind(1, x, x^2, y, y^2)
  betas_full = solve(t(XY_full) %*% XY_full) %*% t(XY_full) %*% W
  w_hat_full = XY_full %*% betas_full
  RSS_full = sum((W - w_hat_full)^2)
  
  # Reduced model: Regressing W only on X and X^2
  XY_reduced = cbind(1, x, x^2)
  betas_reduced = solve(t(XY_reduced) %*% XY_reduced) %*% t(XY_reduced) %*% W
  w_hat_reduced = XY_reduced %*% betas_reduced
  RSS_reduced = sum((W - w_hat_reduced)^2)
  
  # F-test
  F_statistic = ((RSS_reduced - RSS_full) /
                   (length(betas_full) - length(betas_reduced))) /
    (RSS_full / (length(W) - length(betas_full)))
  p_value = 1 - pf(F_statistic,
                   df1 = length(betas_full) - length(betas_reduced),
                   df2 = length(W) - length(betas_full))
  return(p_value)
}

PS2q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```


### Wu-Fuller WA Test

As another special case of the Hausman (1978) misspecification regression test, Wu \& Fuller (2005) extend Dumouchel \& Duncan (1983) model but change how $X$ is transformed in the regression. Consider the regression $$Y = X \beta + \widetilde{X} \widetilde{\beta} + \widetilde{\varepsilon},$$ where $\widetilde{X} = QX$, $Q = \text{diag}(q_1, q_2, \hdots, q_n)$, and $q_i = w_i \hat{w}^{-1}(x_i)$ where $\hat{w}(x_i)$ is estimated from the regression of $w_i$ on $f(x_i)$. Testing the model with the null hypothesis $H_0: \gamma = 0$ determines the impact of $W$ on $Y$ after removing the information contained in $X$ as $q_i$ are the predictable factors of weight $W_i$ by $X_i$.

```{r}
WF_WA_test = function(data, y, x, wts) {
  W = wts

  # Auxiliary Regression
  X_design = cbind(1, x, x^2)
  etas = solve(t(X_design) %*% X_design) %*% t(X_design) %*% W
  W_hat = X_design %*% etas
  q = W / W_hat
    
  # Full Regression
  X_tilde = diag(as.vector(q)) %*% x
  XQ_design = cbind(1, x, X_tilde)
  betas_full = solve(t(XQ_design) %*% XQ_design) %*% t(XQ_design) %*% y
  y_hat_full = XQ_design %*% betas_full
  RSS_full = sum((y - y_hat_full)^2)
  
  # Reduced Regression
  X = cbind(1, x)
  betas_reduced = solve(t(X) %*% X) %*% t(X) %*% y
  y_hat_reduced = X %*% betas_reduced
  RSS_reduced = sum((y - y_hat_reduced)^2)
  
  # F-test - Testing whether beta coefficient for X_tilde is stat sig
  F_statistic = ((RSS_reduced - RSS_full) /
                   (length(betas_full) - length(betas_reduced))) /
    (RSS_full / (length(W) - length(betas_full)))
  p_value = 1 - pf(F_statistic,
                   df1 = length(betas_full) - length(betas_reduced),
                   df2 = length(W) - length(betas_full))
  return(p_value)
}

WF_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```


# Other

### Pfeffermann-Sverchkov Other Test - Estimating Equations

Pfeffermann \& Sverchkov (2003) proposed a test that uses the estimating equations to estimate $\beta$ by an auxiliary regression model for $W$ on some function of $X$ with parameter $\eta$. The unweighted estimating function $$\delta_i(\beta) = X_i (Y_i - X_i^T \beta), i \in S.$$ Define $\hat{W}_i$ as the fitted value of the regression, $q_i = \frac{W_i}{\hat{W}_i}$, and $R(X_i; \beta) = \delta_i(\beta) - q_i \delta_i(\beta)$. Thus, the null hypothesis is $H_0: E(R(X_i; \beta)) = 0$. The sampling weight means $E(R(X_i; \beta))$ can be tested by a Hotelling statistic $$\frac{n - p}{p} \bar{R}_n^{-T} \hat{\Sigma}_{R,n}^{-1} \bar{R}_n,$$ where $\bar{R}_n$ is the sample mean and $\hat{\Sigma}_{R,n}$ is the sample variance matrix of $R(X_i; \hat{\beta}_u)$ with $i \in S$. The statistic then approximately follows an $F$ distribution with ($p, n - p$) degrees of freedom under the null hypothesis. 

```{r}
PS3_test = function(data, y, x, wts) {
  W = wts
  
  # Auxiliary Regression
  X_design = cbind(1, x)
  etas = solve(t(X_design) %*% X_design) %*% t(X_design) %*% W
  W_hat = X_design %*% etas
  q = W / W_hat
  
  # Estimating Regression
  X = cbind(1, x)
  betas = solve(t(X) %*% X) %*% t(X) %*% y
  
  # Unweighted estimating function
  delta = rep(NA, length(y))
  
  for (i in 1:length(y)) {
    delta[i] = x[i] * (y[i] - X[i,] %*% betas)
  }
  
  R_x = delta - q * delta
  
  # Hotelling Test Statistic
  F_statistic = ((length(y) - length(betas[-1])) / length(betas[-1])) *
    t(mean(R_x)) %*% solve(cov(R_x)) %*% mean(R_x)
  p_value = 1 - pf(F_statistic, df1 = length(betas[-1]), df2 = length(y) - length(betas[-1]))
  return(p_value)
}

PS3_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)

PS3q_test = function(data, y, x, wts) {
  W = wts
  
  # Auxiliary Regression
  X_design = cbind(1, x, x^2)
  etas = solve(t(X_design) %*% X_design) %*% t(X_design) %*% W
  W_hat = X_design %*% etas
  q = W / W_hat
  
  # Estimating Regression
  X = cbind(1, x)
  betas = solve(t(X) %*% X) %*% t(X) %*% y
  
  # Unweighted estimating function
  delta = rep(NA, length(y))
  
  for (i in 1:length(y)) {
    delta[i] = x[i] * (y[i] - X[i,] %*% betas)
  }
  
  R_x = delta - q * delta
  
  # Hotelling Test Statistic
  F_statistic = ((length(y) - length(betas[-1])) / length(betas[-1])) *
    t(mean(R_x)) %*% solve(cov(R_x)) %*% mean(R_x)
  p_value = 1 - pf(F_statistic, df1 = length(betas[-1]), df2 = length(y) - length(betas[-1]))
  return(p_value)
}

PS3q_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```


### Pfeffermann-Nathan Predictive Power Test

```{r}
PN_test = function(data, y, x, wts, est_split = 0.5) {
  index = sample(1:nrow(data), floor(est_split * length(x)))
  
  # Unweighted Regression
  X = cbind(1, x[index])
  betas_u = solve(t(X) %*% X) %*% t(X) %*% y[index]
  y_val_u = cbind(1, x[-index]) %*% betas_u
  v_u = y[-index] - y_val_u
    
  # Weighted Regression 
  W = diag(wts[index])
  betas_w = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y[index]
  y_val_w = cbind(1, x[-index]) %*% betas_w
  v_w = y[-index] - y_val_w
  
  # Standard Z-test
  D = v_u^2 - v_w^2
  Z = (mean(D) - 0) / (sd(D) / sqrt(length(y)))
  p_value = 2 * pnorm(abs(Z), lower.tail = FALSE)
  return(p_value)
}
PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.7)

PN_test_new = function(data, y, x, wts, est_split = 0.5) {
  
}
PN_test_new(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.7)



y = samp$y
x = samp$x
wts = samp$w
data = samp
index = sample(1:nrow(data), floor(est_split * length(x)))
  
# Unweighted Regression
X = cbind(1, x[index])
betas_u = solve(t(X) %*% X) %*% t(X) %*% y[index]
y_val_u = cbind(1, x[-index]) %*% betas_u
v_u = y[-index] - y_val_u
  
# Weighted Regression 
W = diag(wts[index])
betas_w = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y[index]
y_val_w = cbind(1, x[-index]) %*% betas_w
v_w = y[-index] - y_val_w

# Standard Z-test
D = v_u^2 - v_w^2
Z = (mean(D) - 0) / (sd(D) / sqrt(length(y)))
p_value = 2 * pnorm(abs(Z), lower.tail = FALSE)
p_value


lmw = lm(y ~ x, data = samp[-index,], weights = w)
lmo = lm(y ~ x, data = samp[-index,])

print(v_w)
```






```{r}
####################################3

#For determining when each test performs well, do a heatmap and facit by extreme case
```


### Breidt Likelihood-Ratio Test

```{r}
LR_test = function(data, y, x, wts) {
  X = cbind(1, x)
  
  # Define the log-likelihood function
  logLike <- function(params, y, X, wts) {
    beta <- params[1:ncol(X)]
    sigma2 <- exp(params[ncol(X)+1])  # we use exp to ensure sigma^2 is positive
    mu <- X %*% beta
    n <- length(y)
    logLik <- -0.5 * log(2 * pi * sigma2) * sum(wts) - 0.5 * sum(wts * (y - mu)^2) / sigma2
    return(-logLik)  # We negate because optim() minimizes by default
  }
  
  initial <- c(rep(0, ncol(X)), log(var(y)))
  
  # theta_u estimates
  res <- optim(initial, logLike, y = y, X = X, wts = rep(sum(wts) / length(wts), length(wts)))
  beta_u <- res$par[1:ncol(X)]
  sigma_sq_u <- exp(res$par[ncol(X)+1])
  theta_u = c(beta_u, sigma_sq_u)
  loglike_u = res$value
  
  # theta-w estimates
  res <- optim(initial, logLike, y = y, X = X, wts = wts)
  beta_w <- res$par[1:ncol(X)]
  sigma_sq_w <- exp(res$par[ncol(X)+1])
  theta_w = c(beta_w, sigma_sq_w)
  loglike_w = res$value
  
  # True parameter
  X = cbind(1, x)
  beta_true = solve(t(X) %*% X) %*% t(X) %*% y
  sigma_sq = mean((y - X %*% beta_true)^2)
  theta_true = c(beta_true, sigma_sq)
  
  # Test Statistic
  null_weights = rep(sum(wts) / length(wts), length(wts))
  l_u_theta_u = 0.5 * log(2 * pi * sigma_sq_u) * sum(null_weights) -
    0.5 * sum(null_weights * (y - X %*% beta_u)^2) / sigma_sq_u
  l_u_theta_w = 0.5 * log(2 * pi * sigma_sq_w) * sum(null_weights) - 
    0.5 * sum(null_weights * (y - X %*% beta_w)^2) / sigma_sq_w
  
  test_statistic = 2 * (l_u_theta_u - l_u_theta_w)
  
  # Asymptotic Test Distribution (Using Fisher Information matrix)
  J_u = diag(c(sum(t(X) %*% X / sigma_sq), # with respect to X, not X_i
           1 / (2 * (sigma_sq)^2)))
  J_w = diag(c(sum(t(X) %*% diag(wts) %*% X / sigma_sq),
           sum(wts) / (2 * length(wts) * (sigma_sq)^2)))
  K_w = diag(c(sum(t(X) %*% diag(wts^2) %*% X / sigma_sq),
           sum(wts^2) / (2 * length(wts) * (sigma_sq)^2)))
  
  Gamma = solve(J_w) %*% K_w %*% solve(J_w) - solve(J_u)
  
  stat_matrix = t(sqrt(Gamma)) %*% J_u %*% sqrt(Gamma)
  eigenvalues = eigen(stat_matrix, only.values = TRUE)$values
  
  df = 1:length(eigenvalues)
  chi_square_rv = sapply(df, function(d) rchisq(10000, df = d))
  asymptotic_comb = (1 / chi_square_rv) %*% eigenvalues
  ecdf_LR = ecdf(asymptotic_comb)
  p_value = 1 - ecdf_LR(test_statistic)
  return(p_value)
}

LR_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
```


### Confidence Intervals

```{r, eval = FALSE}
confint_test = function(data, y, x, wts, alpha = 0.05) {
  X = cbind(1, x)
  
  # Unweighted beta CI
  betas_u = solve(t(X) %*% X) %*% t(X) %*% y
  residuals_u = y - X %*% betas_u
  MSE_u = sum(residuals_u^2) / (nrow(X) - ncol(X))
  vcov_matrix_u = MSE_u * solve(t(X) %*% X)
  se_u = sqrt(diag(vcov_matrix_u))
  t_value = qt(1 - alpha / 2, df = nrow(X) - ncol(X))
  CI_u = cbind(betas_u - t_value * se_u, betas_u + t_value * se_u)
  beta_1_u = CI_u[2,]
  
  # Weighted beta CI
  W = diag(wts)
  betas_w = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y
  residuals_w = y - X %*% betas_w
  MSE_w = sum(residuals_w^2) / (nrow(X) - ncol(X))
  vcov_matrix_w = MSE_w * solve(t(X) %*% X)
  se_w = sqrt(diag(vcov_matrix_w))
  t_value = qt(1 - alpha / 2, df = nrow(X) - ncol(X))
  CI_w = cbind(betas_w - t_value * se_w, betas_w + t_value * se_w)
  beta_1_w = CI_w[2,]
  
  # Determine overlap
  overlap = !(beta_1_w[2] < beta_1_u[1] | beta_1_u[2] < beta_1_w[1])
  return(as.numeric(overlap)) # if overlap, returns 1 so fail to reject the null
}

confint_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, alpha = 0.10)
```



### Permutation

<!---- TO-DO: Determine how to efficiently calculate permutation statistic ---->
```{r, include = FALSE, eval = FALSE}
weight_perm_test <- function(y, x, w, B) {
  stat_stor = rep(NA, B)
  data = data.frame(x = x, y = y, w = w)
  # Permutating
  for (b in 1:B) {
    shuffled_w = sample(w)
    b_data = data.frame(y = y, x = x, w = shuffled_w, id = length(w))
    
    # Unweighted Regression
    unweighted = lm(y ~ x, data = b_data)
    beta_u = unweighted$coefficients[2] # faster to manually calculate beta_1
    #beta_u = sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)
    
    # Weighted Regression
    design = svydesign(id = ~1, weights = ~w, fpc = ~rep(N, nrow(b_data)),
                       data = b_data)
    weighted = svyglm(y ~ x, design = design)
    beta_w = weighted$coefficients[2]
    
    stat_stor[b] = beta_w - beta_u
  }
  
  # Estimating test statistic
  act_betau = lm(y ~ x, data = data)$coefficients[2]
  
  act_design = svydesign(id = ~1, weights = ~w, fpc = ~rep(N, nrow(data)),
                       data = data)
  act_betaw = svyglm(y ~ x, design = act_design)$coefficients[2]
  est_stat = act_betaw - act_betau
  
  # Calculating p-value
  dist = ecdf(stat_stor)
  p = dist(est_stat)
  pvalue = 2 * min(p, 1 - p)
  
  # return list for distribution and p-value
  return(pvalue)
}

weight_perm_test = function(data, y, x, wts, B) {
  
}
B = 100
stat_stor = rep(NA, B)

data = samp

for (b in 1:B) {
  b_data = mutate(data, w = sample(w))
  
  # Unweighted Regression
  X = cbind(1, b_data$x)
  Y = b_data$y
  betas_u = solve(t(X) %*% X) %*% t(X) %*% Y
  
  # Weighted Regression
  W = diag(x = b_data$w)
  betas_w = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% Y
  
  stat_stor[b] = betas_w[2] - betas_u[2]
}

# Calculating Test Statistic
# Unweighted Regression
X = cbind(1, data$x)
Y = data$y
betas_u = solve(t(X) %*% X) %*% t(X) %*% Y

# Weighted Regression
W = diag(x = data$w)
betas_w = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% Y
  
est_stat = betas_w[2] - betas_u[2]

# Calculating p-value
dist = ecdf(stat_stor)
p = dist(est_stat)
p_value = 2 * min(p, 1 - p)
p_value


# hist(stat_stor)
p_value <- sum(abs(stat_stor) >= abs(est_stat)) / length(stat_stor)
p_value


hist(stat_stor)
```


# Simulation 

### Study 1

```{r}
set.seed(51483464)
B = 1000 # 10000 for large study # 1000 for normal

N <- 3000
n <- c(100, 200)
sigma <- c(0.1, 0.2)
alpha <- c(0.0, 0.2, 0.4, 0.6)
delta <- c(1.5, 1)
cases <- expand_grid(N, n, sigma, delta, alpha)

columns = c("case", "iteration", "DD", "PN", "HP", "PS1", "PS1q", "PS2", "PS2q", 
            "PS3", "WF", "LR")
results = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(results) = columns

for (case in 1:nrow(cases)) {
  DD = PN = HP = PS1 = PS1q = PS2 = PS2q = PS3 = WF = LR = rep(NA, B)
  case_storage = data.frame(iteration = seq_len(B), DD, PN, HP, PS1, PS1q,
                            PS2, PS2q, PS3, WF, LR)
  for (b in 1:B) {
    pop = generate_data_study1(N = cases$N[case],
                               sigma = cases$sigma[case],
                               alpha = cases$alpha[case],
                               delta = cases$delta[case])
    samp = generate_sample_brewer(pop, w = pop$w, n = cases$n[case])
    
    case_storage$DD[b] = DD_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PN[b] = PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.5)
    case_storage$HP[b] = HP_DC_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS1[b] = PS1_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS1q[b] = PS1q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS2[b] = PS2_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS2q[b] = PS2q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS3[b] = PS3_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$WF[b] = WF_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$LR[b] = LR_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
  }
  
  results = rbind(results, cbind(case, case_storage))
  print(case)
}
write.csv(results, "st1_results.csv")
#write.csv(results, "st1_results_large.csv")
```

```{r}
results = read.csv("st1_results.csv")
#results = read.csv("st1_results_large.csv")

reject = results %>%
  mutate(HP = case_when(HP <= 0.05 ~ 1, TRUE ~ 0),
         DD = case_when(DD <= 0.05 ~ 1, TRUE ~ 0),
         PS1 = case_when(PS1 <= 0.05 ~ 1, TRUE ~ 0),
         PS1q = case_when(PS1q <= 0.05 ~ 1, TRUE ~ 0),
         PS2 = case_when(PS2 <= 0.05 ~ 1, TRUE ~ 0),
         PS2q = case_when(PS2q <= 0.05 ~ 1, TRUE ~ 0),
         PS3 = case_when(PS3 <= 0.05 ~ 1, TRUE ~ 0),
         WF = case_when(WF <= 0.05 ~ 1, TRUE ~ 0),
         LR = case_when(LR <= 0.05 ~ 1, TRUE ~ 0),
         PN = case_when(PN <= 0.05 ~ 1, TRUE ~ 0)) %>%
  select(-iteration) %>%
  group_by(case) %>%
  summarize(across(everything(), mean)) %>%
  mutate(HP = format(round(HP * 100, 1), nsmall = 1),
         DD = format(round(DD * 100, 1), nsmall = 1),
         PS1 = format(round(PS1 * 100, 1), nsmall = 1),
         PS1q = format(round(PS1q * 100, 1), nsmall = 1),
         PS2 = format(round(PS2 * 100, 1), nsmall = 1),
         PS2q = format(round(PS2q * 100, 1), nsmall = 1),
         PS3 = format(round(PS3 * 100, 1), nsmall = 1),
         WF = format(round(WF * 100, 1), nsmall = 1),
         LR = format(round(LR * 100, 1), nsmall = 1),
         PN = format(round(PN * 100, 1), nsmall = 1))

reject_table = cbind(cases, reject) %>% select(-c(N, case, X))
reject_table

#write.csv(reject_table, "recent_sim1.csv")
#write.csv(reject_table, "recent_sim1_large.csv")

hi = read.csv("recent_sim1.csv")
```

### Study 2

```{r}
set.seed(51483464)
B = 10000 # 1000

N <- 3000
n <- c(100, 200)
sigma <- c(0.1)
alpha <- c(0, 0.5, 1.0, 1.5)
cases <- expand_grid(N, n, sigma, alpha)

columns = c("case", "iteration", "DD", "PN", "HP", "PS1", "PS1q", "PS2", "PS2q", 
            "PS3", "WF", "LR")
results = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(results) = columns

for (case in 1:nrow(cases)) {
  DD = PN = HP = PS1 = PS1q = PS2 = PS2q = PS3 = WF = LR = rep(NA, B)
  case_storage = data.frame(iteration = seq_len(B), DD, PN, HP, PS1, PS1q,
                            PS2, PS2q, PS3, WF, LR)
  for (b in 1:B) {
    pop = generate_data_study2(N = cases$N[case],
                               sigma = cases$sigma[case],
                               alpha = cases$alpha[case])
    samp = generate_sample_brewer(pop, w = pop$w, n = cases$n[case])
    
    case_storage$DD[b] = DD_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PN[b] = PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.5)
    case_storage$HP[b] = HP_DC_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS1[b] = PS1_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS1q[b] = PS1q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS2[b] = PS2_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS2q[b] = PS2q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS3[b] = PS3_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$WF[b] = WF_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$LR[b] = LR_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
  }
  
  results = rbind(results, cbind(case, case_storage))
  print(case)
}
# write.csv(results, "st2_results.csv")
write.csv(results, "st2_results_large.csv")
```

```{r}
#results = read.csv("st2_results_large.csv")
results = read.csv("st2_results.csv")

reject = results %>%
  mutate(HP = case_when(HP <= 0.05 ~ 1, TRUE ~ 0),
         DD = case_when(DD <= 0.05 ~ 1, TRUE ~ 0),
         PS1 = case_when(PS1 <= 0.05 ~ 1, TRUE ~ 0),
         PS1q = case_when(PS1q <= 0.05 ~ 1, TRUE ~ 0),
         PS2 = case_when(PS2 <= 0.05 ~ 1, TRUE ~ 0),
         PS2q = case_when(PS2q <= 0.05 ~ 1, TRUE ~ 0),
         PS3 = case_when(PS3 <= 0.05 ~ 1, TRUE ~ 0),
         WF = case_when(WF <= 0.05 ~ 1, TRUE ~ 0),
         LR = case_when(LR <= 0.05 ~ 1, TRUE ~ 0),
         PN = case_when(PN <= 0.05 ~ 1, TRUE ~ 0)) %>%
  select(-iteration) %>%
  group_by(case) %>%
  summarize(across(everything(), mean)) %>%
  mutate(HP = format(round(HP * 100, 1), nsmall = 1),
         DD = format(round(DD * 100, 1), nsmall = 1),
         PS1 = format(round(PS1 * 100, 1), nsmall = 1),
         PS1q = format(round(PS1q * 100, 1), nsmall = 1),
         PS2 = format(round(PS2 * 100, 1), nsmall = 1),
         PS2q = format(round(PS2q * 100, 1), nsmall = 1),
         PS3 = format(round(PS3 * 100, 1), nsmall = 1),
         WF = format(round(WF * 100, 1), nsmall = 1),
         LR = format(round(LR * 100, 1), nsmall = 1),
         PN = format(round(PN * 100, 1), nsmall = 1))

reject_table = cbind(cases, reject) %>% select(-c(N, case))
reject_table

#write.csv(reject_table, "recent_sim2.csv")
write.csv(reject_table, "recent_sim2_large.csv")
```

```{r}
heatmap_data = results %>%
  mutate(HP = case_when(HP <= 0.05 ~ 1, TRUE ~ 0),
         DD = case_when(DD <= 0.05 ~ 1, TRUE ~ 0),
         PS1 = case_when(PS1 <= 0.05 ~ 1, TRUE ~ 0),
         PS1q = case_when(PS1q <= 0.05 ~ 1, TRUE ~ 0),
         PS2 = case_when(PS2 <= 0.05 ~ 1, TRUE ~ 0),
         PS2q = case_when(PS2q <= 0.05 ~ 1, TRUE ~ 0),
         PS3 = case_when(PS3 <= 0.05 ~ 1, TRUE ~ 0),
         WF = case_when(WF <= 0.05 ~ 1, TRUE ~ 0),
         LR = case_when(LR <= 0.05 ~ 1, TRUE ~ 0),
         PN = case_when(PN <= 0.05 ~ 1, TRUE ~ 0)) %>%
  cbind(., cases) %>%
  filter(n %in% c(100,200), alpha %in% c(0.5, 1.0)) %>%
  select(-c(X, case, PS3q, iteration, LR, PN, N, sigma)) #%>%
  #melt(id.vars = "iteration")


help = results %>%
  mutate(HP = case_when(HP <= 0.05 ~ 1, TRUE ~ 0),
         DD = case_when(DD <= 0.05 ~ 1, TRUE ~ 0),
         PS1 = case_when(PS1 <= 0.05 ~ 1, TRUE ~ 0),
         PS1q = case_when(PS1q <= 0.05 ~ 1, TRUE ~ 0),
         PS2 = case_when(PS2 <= 0.05 ~ 1, TRUE ~ 0),
         PS2q = case_when(PS2q <= 0.05 ~ 1, TRUE ~ 0),
         PS3 = case_when(PS3 <= 0.05 ~ 1, TRUE ~ 0),
         WF = case_when(WF <= 0.05 ~ 1, TRUE ~ 0),
         LR = case_when(LR <= 0.05 ~ 1, TRUE ~ 0),
         PN = case_when(PN <= 0.05 ~ 1, TRUE ~ 0)) %>%
  filter(case %in% c(2,3,6,7)) %>%
  select(-c(X, PS3q, iteration, LR, PN))


head(heatmap_data)

hi = cor(heatmap_data)

why = rbind(cor(filter(help, case == 2) %>% select(-case)),
      cor(filter(help, case == 3) %>% select(-case)),
      cor(filter(help, case == 6) %>% select(-case)),
      cor(filter(help, case == 7) %>% select(-case)))


case4 = cbind(as.data.frame(as.table(cor(filter(help, case == 7) %>% select(-case)))),
              )


library(reshape)

ggplot(data = as.data.frame(as.table(cor(heatmap_data)))) + 
  geom_tile(aes(x = Var1, y = Var2, fill = Freq)) +
    scale_fill_gradient(low = "white", high = "black")



heatmap(hi,
        col = colorRampPalette(c("blue", "white", "red"))(100),
        main = "Correlation Heatmap",
        xlab = "Variables",
        ylab = "Variables")
```


### Study 3

```{r}
set.seed(51483464)
B = 10000 # 1000

N <- 3000
n <- c(100, 200)
psi <- c(0.0, 0.1, 0.2, 0.3)
alpha <- c(1.00, 0.75, 0.50, 0.25)
cases <- expand_grid(N, n, alpha, psi)

columns = c("case", "iteration", "DD", "PN", "HP", "PS1", "PS1q", "PS2", "PS2q", 
            "PS3", "PS3q", "WF", "LR")
results = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(results) = columns

for (case in 1:nrow(cases)) {
  DD = PN = HP = PS1 = PS1q = PS2 = PS2q = PS3 = WF = LR = rep(NA, B)
  case_storage = data.frame(iteration = seq_len(B), DD, PN, HP, PS1, PS1q,
                            PS2, PS2q, PS3, WF, LR)
  for (b in 1:B) {
    pop = generate_data_study3(N = cases$N[case],
                               alpha = cases$alpha[case],
                               psi = cases$psi[case])
    samp = generate_sample_poisson(pop, w = pop$w, n = cases$n[case])
    
    case_storage$DD[b] = DD_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PN[b] = PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.5)
    case_storage$HP[b] = HP_DC_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS1[b] = PS1_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS1q[b] = PS1q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS2[b] = PS2_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS2q[b] = PS2q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS3[b] = PS3_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$WF[b] = WF_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$LR[b] = LR_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
  }
  
  results = rbind(results, cbind(case, case_storage))
  print(case)
}
#write.csv(results, "st3_results.csv")
write.csv(results, "st3_results_large.csv")
```

```{r}
results = read.csv("st3_results_large.csv")
# results = read.csv("st3_results.csv")

reject = results %>%
  mutate(HP = case_when(HP <= 0.05 ~ 1, TRUE ~ 0),
         DD = case_when(DD <= 0.05 ~ 1, TRUE ~ 0),
         PS1 = case_when(PS1 <= 0.05 ~ 1, TRUE ~ 0),
         PS1q = case_when(PS1q <= 0.05 ~ 1, TRUE ~ 0),
         PS2 = case_when(PS2 <= 0.05 ~ 1, TRUE ~ 0),
         PS2q = case_when(PS2q <= 0.05 ~ 1, TRUE ~ 0),
         PS3 = case_when(PS3 <= 0.05 ~ 1, TRUE ~ 0),
         WF = case_when(WF <= 0.05 ~ 1, TRUE ~ 0),
         LR = case_when(LR <= 0.05 ~ 1, TRUE ~ 0),
         PN = case_when(PN <= 0.05 ~ 1, TRUE ~ 0)) %>%
  select(-iteration) %>%
  group_by(case) %>%
  summarize(across(everything(), mean)) %>%
  mutate(HP = format(round(HP * 100, 1), nsmall = 1),
         DD = format(round(DD * 100, 1), nsmall = 1),
         PS1 = format(round(PS1 * 100, 1), nsmall = 1),
         PS1q = format(round(PS1q * 100, 1), nsmall = 1),
         PS2 = format(round(PS2 * 100, 1), nsmall = 1),
         PS2q = format(round(PS2q * 100, 1), nsmall = 1),
         PS3 = format(round(PS3 * 100, 1), nsmall = 1),
         WF = format(round(WF * 100, 1), nsmall = 1),
         LR = format(round(LR * 100, 1), nsmall = 1),
         PN = format(round(PN * 100, 1), nsmall = 1))

reject_table = cbind(cases, reject) %>% select(-c(N, case))
reject_table

#write.csv(reject_table, "recent_sim3.csv")
write.csv(reject_table, "recent_sim3_large.csv")
```

## Suspicion

```{r}
set.seed(51483464)
B = 1000

N <- 3000
n <- c(100, 200)
psi <- c(0.0, 0.1, 0.2, 0.3)
alpha <- c(1.00, 0.75, 0.50, 0.25)
cases <- expand_grid(N, n, alpha, psi)

columns = c("case", "iteration", "DD", "PN", "HP", "PS1", "PS1q", "PS2", "PS2q", 
            "PS3", "PS3q", "WF", "LR")
results = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(results) = columns

for (case in 1:nrow(cases)) {
  DD = PN = HP = PS1 = PS1q = PS2 = PS2q = PS3 = WF = LR = rep(NA, B)
  case_storage = data.frame(iteration = seq_len(B), DD, PN, HP, PS1, PS1q,
                            PS2, PS2q, PS3, WF, LR)
  for (b in 1:B) {
    pop = generate_data_study3(N = cases$N[case],
                               alpha = cases$alpha[case],
                               psi = cases$psi[case])
    samp = generate_sample_poisson_suspicious(pop, w = pop$w, n = cases$n[case])
    
    case_storage$DD[b] = DD_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PN[b] = PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.5)
    case_storage$HP[b] = HP_DC_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS1[b] = PS1_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS1q[b] = PS1q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS2[b] = PS2_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS2q[b] = PS2q_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PS3[b] = PS3_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$WF[b] = WF_WA_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$LR[b] = LR_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
  }
  
  results = rbind(results, cbind(case, case_storage))
  print(case)
}
write.csv(results, "st3_suspicious.csv")
```

```{r}
results = read.csv("st3_suspicious.csv")

reject = results %>%
  mutate(HP = case_when(HP <= 0.05 ~ 1, TRUE ~ 0),
         DD = case_when(DD <= 0.05 ~ 1, TRUE ~ 0),
         PS1 = case_when(PS1 <= 0.05 ~ 1, TRUE ~ 0),
         PS1q = case_when(PS1q <= 0.05 ~ 1, TRUE ~ 0),
         PS2 = case_when(PS2 <= 0.05 ~ 1, TRUE ~ 0),
         PS2q = case_when(PS2q <= 0.05 ~ 1, TRUE ~ 0),
         PS3 = case_when(PS3 <= 0.05 ~ 1, TRUE ~ 0),
         WF = case_when(WF <= 0.05 ~ 1, TRUE ~ 0),
         LR = case_when(LR <= 0.05 ~ 1, TRUE ~ 0),
         PN = case_when(PN <= 0.05 ~ 1, TRUE ~ 0)) %>%
  select(-iteration) %>%
  group_by(case) %>%
  summarize(across(everything(), mean)) %>%
  mutate(HP = format(round(HP * 100, 1), nsmall = 1),
         DD = format(round(DD * 100, 1), nsmall = 1),
         PS1 = format(round(PS1 * 100, 1), nsmall = 1),
         PS1q = format(round(PS1q * 100, 1), nsmall = 1),
         PS2 = format(round(PS2 * 100, 1), nsmall = 1),
         PS2q = format(round(PS2q * 100, 1), nsmall = 1),
         PS3 = format(round(PS3 * 100, 1), nsmall = 1),
         WF = format(round(WF * 100, 1), nsmall = 1),
         LR = format(round(LR * 100, 1), nsmall = 1),
         PN = format(round(PN * 100, 1), nsmall = 1))

reject_table = cbind(cases, reject) %>% select(-c(N, case)) 
reject_table

write.csv(reject_table, "suspicious_sim3.csv")
```




# Mini-Simulation Studies

```{r}
set.seed(51483464)
B = 1000

N <- 3000
n <- c(100, 200)
sigma <- c(0.1, 0.2)
alpha <- c(0, 0.2, 0.4, 0.6)
delta <- c(1.5, 1)
cases <- expand_grid(N, n, sigma, delta, alpha)

columns = c("case", "iteration", "PN", "PN_new") ###
mini_results = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(mini_results) = columns

for (case in 1:nrow(cases)) {
  PN = PN_new = rep(NA, B) ###
  case_storage = data.frame(iteration = seq_len(B), PN, PN_new) ###
  for (b in 1:B) {
    pop = generate_data_study1(N = cases$N[case],
                               sigma = cases$sigma[case],
                               alpha = cases$alpha[case],
                               delta = cases$delta[case])
    samp = generate_sample_brewer(pop, w = pop$w, n = cases$n[case])
    
    case_storage$PN[b] = PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    case_storage$PN_new[b] = PN_test_new(data = samp, y = samp$y, x = samp$x, wts = samp$w)
    
    ###
    #case_storage$PN50[b] = PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.5)
    #case_storage$PN90[b] = PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.9)
    #case_storage$PN10[b] = PN_test(data = samp, y = samp$y, x = samp$x, wts = samp$w, est_split = 0.1)
  }
  
  mini_results = rbind(mini_results, cbind(case, case_storage))
  print(case)
}

reject_mini = mini_results %>% ###
  mutate(PN = case_when(PN <= 0.05 ~ 1, TRUE ~ 0),
         PN_new = case_when(PN_new <= 0.05 ~ 1, TRUE ~ 0)) %>%
  select(-iteration) %>%
  group_by(case) %>%
  summarize(across(everything(), mean))

reject_mini_table = cbind(cases, reject_mini) %>% select(-c(N, case))
reject_mini_table
```

### E(W_i) Derivation

```{r}
psi_list <- c(0.0, 0.1, 0.2, 0.3)
alpha_list <- c(1.00, 0.75, 0.50, 0.25)

psi = psi_list[4]
alpha = alpha_list[4]
beta = 2 - alpha
eta_x = 0.110367
sigma_v = sqrt(0.5 * (psi^2 + (1 - psi)^2))

study3_integral = function(x) {
   (x / sigma_v) / sqrt(2 * pi) * exp(-(x / sigma_v)^2 / 2)
}

mean = alpha * eta_x + beta * (0.025 * pnorm(0.2 / sigma_v) +
  0.475 * integrate(study3_integral, lower = 0.2, upper = 1.2)$value -
  0.07 * (pnorm(1.2 / sigma_v) - pnorm(0.2 / sigma_v)) +
  0.5 * (1 - pnorm(1.2 / sigma_v)))
mean
```

### Hausman-Pfeffermann DC Test Simulation

Showcasing the rate of negative variance estimates with Wang et al's simulation study 1

```{r}
HP_DC_var_sim = function(data, y, x, wts) {
  x = samp$x
  y = samp$y
  wts = samp$w
  
  X = cbind(1, x)
  betas_u = solve(t(X) %*% X) %*% t(X) %*% y
  
  # Weighted Regression
  W =  diag(x = wts, nrow = length(wts), ncol = length(wts))
  betas_w = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y
  
  # Calculate sigma^2 estimate from OLS under null
  y_hat = X %*% betas_u
  residuals = y - y_hat
  SSE_u = sum(residuals^2)
  sigma_sq_hat_u = SSE_u / (length(y) - length(betas_u) - 1)
  v_beta_u = sigma_sq_hat_u * solve(t(X) %*% X)
  
  SSE_w = sum((y - X %*% betas_w)^2)
  sigma_sq_hat_w = SSE_w / (length(y) - length(betas_w) - 1)
  v_beta_w = sigma_sq_hat_w * solve(t(X) %*% X) # SHould i include the W term?
  
  # Calculate variance
  V_hat = v_beta_w[2,2] - v_beta_u[2,2]
  return(V_hat)
}

HP_DC_var_sim(data = samp, y = samp$y, x = samp$x, wts = samp$w)



```

```{r}
set.seed(51483464)
B = 500

N <- 3000
n <- c(25, 50, 75, 100, 150, 200)
sigma <- c(0.1)
alpha <- c(0, 0.2, 0.4, 0.6)
delta <- c(1)
cases <- expand_grid(N, n, sigma, delta, alpha)

columns = c("case", "iteration", "HP_DC")
hp_var = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(hp_var) = columns

for (case in 1:nrow(cases)) {
  CI = rep(NA, B)
  case_storage = data.frame(iteration = seq_len(B), CI)
  for (b in 1:B) {
    pop = generate_data_study1(N = cases$N[case],
                               sigma = cases$sigma[case],
                               alpha = cases$alpha[case],
                               delta = cases$delta[case])
    samp = generate_sample_brewer(pop, w = pop$w, n = cases$n[case])
    
    case_storage$HP_DC[b] = HP_DC_var_sim(data = samp, y = samp$y, x = samp$x, wts = samp$w)
  }
  
  hp_var = rbind(hp_var, cbind(case, case_storage))
  print(case)
}

reject_hp_var = hp_var %>%
  mutate(HP_DC = case_when(HP_DC <= 0 ~ 1, TRUE ~ 0)) %>%
  select(-iteration) %>%
  group_by(case) %>%
  summarize(negative_rate = mean(HP_DC))
```

